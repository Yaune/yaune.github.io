<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="[TOC] Kafka基础1. 概述Kafka 是一种分布式的，基于发布 &#x2F; 订阅的消息系统。 Kafka 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于 Zookeeper 协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于 hadoop 的批处理系统、低延迟的实时系统、Storm&#x2F;S">
<meta property="og:type" content="article">
<meta property="og:title" content="ShiftJava&#x2F;L 消息队列&#x2F;D Kafka">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;index.html">
<meta property="og:site_name" content="路漫漫其修远兮">
<meta property="og:description" content="[TOC] Kafka基础1. 概述Kafka 是一种分布式的，基于发布 &#x2F; 订阅的消息系统。 Kafka 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于 Zookeeper 协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于 hadoop 的批处理系统、低延迟的实时系统、Storm&#x2F;S">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;主题与分区.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;Broker和集群.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;nAcIiT.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;kafka存在文件系统上.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728174836476.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;segment是kafka文件存储的最小单位.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728163144182.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728163249009.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728163330121.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728163404629.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728163529141.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728164036156.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728170112324.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;image-20200728164036156.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;1567511669385.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;kafka-zk.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;1567513386020.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;1567513364566.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;kafka-produce.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;kafka-offset.png">
<meta property="og:updated_time" content="2021-01-06T11:22:32.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;08&#x2F;07&#x2F;ShiftJava&#x2F;L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&#x2F;D%20Kafka&#x2F;assets&#x2F;主题与分区.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","wideth":200,"display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":1},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2021/08/07/ShiftJava/L 消息队列/D Kafka/"/>





  <title>ShiftJava/L 消息队列/D Kafka | 路漫漫其修远兮</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">路漫漫其修远兮</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/08/07/ShiftJava/L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/D%20Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yue">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="路漫漫其修远兮">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">ShiftJava/L 消息队列/D Kafka</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-08-07T19:04:17+08:00">
                2021-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><h5 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h5><p><strong>Kafka 是一种分布式的，基于发布 / 订阅的消息系统。</strong> Kafka 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），<strong>基于 Zookeeper 协调的分布式消息系统</strong>，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于 hadoop 的批处理系统、低延迟的实时系统、Storm/Spark 流式处理引擎， web/nginx 日志、访问日志，消息服务等等。</p>
<p>主要特点如下：</p>
<ul>
<li>以时间复杂度为 O(1) 的方式<strong>提供消息持久化能力</strong>，即使对 TB 级以上数据也能保证<strong>常数时间复杂度的访问性能</strong>。</li>
<li><strong>高吞吐率</strong>。即使在非常廉价的商用机器上也能做到单机支持<strong>每秒 100K 条以上</strong>消息的传输。</li>
<li>支持 Kafka Server 间的消息分区，及分布式消费，同时保证<strong>每个 Partition 内的消息顺序传输</strong>。</li>
<li>同时支持离线数据处理和实时数据处理。</li>
<li>Scale out：支持在线水平扩展。</li>
</ul>
<p>Kafka 是一个分布式流式处理平台。</p>
<p>流平台具有三个关键功能：</p>
<ol>
<li><strong>消息队列</strong>：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。</li>
<li><strong>容错的持久方式存储记录消息流</strong>： Kafka 会把消息<strong>持久化到磁盘</strong>，有效避免了消息丢失的风险。</li>
<li><strong>流式处理平台：</strong> 在消息发布的时候进行处理，Kafka 提供了一个完整的<strong>流式处理类库</strong>。</li>
</ol>
<p>Kafka 的主要应用场景：</p>
<ol>
<li><strong>消息队列</strong> ：建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。</li>
<li><strong>数据处理：</strong> 构建实时的流数据处理程序来转换或处理数据流。</li>
<li><strong>日志收集</strong>：可以用 Kafka 收集各种服务的 <strong>log</strong>，通过 kafka 以<strong>统一接口服务</strong>的方式开放给各种 consumer，例如 hadoop、Hbase、Solr 等。</li>
<li><strong>消息系统</strong>：解耦和生产者和消费者、缓存消息等。</li>
<li><strong>用户活动跟踪</strong>：Kafka 经常被用来记录 web 用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 kafka 的 topic 中，然后订阅者通过订阅这些 topic 来做<strong>实时的监控分析</strong>，或者装载到 hadoop、数据仓库中做<strong>离线分析和挖掘</strong>。</li>
<li><strong>运营指标</strong>：Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。</li>
</ol>
<h5 id="2-主题与分区"><a href="#2-主题与分区" class="headerlink" title="2. 主题与分区"></a>2. 主题与分区</h5><p>在 Kafka 中，<strong>消息</strong>以<strong>主题（Topic）</strong>来分类，每一个<strong>主题</strong>都对应一个 <strong>「消息队列」</strong>，这有点儿类似于数据库中的表。但是如果把所有同类的消息都塞入到一个“中心”队列中，势必缺少可伸缩性，无论是生产者/消费者数目的增加，还是消息数量的增加，都可能耗尽系统的性能或存储。</p>
<img src="assets/主题与分区.png" alt="主题（Topic）与分区（Partition）" style="zoom:50%;" />

<p>使用一个生活中的例子来说明：现在 A 城市生产的某商品需要运输到 B 城市，走的是公路，那么单通道的高速公路不论是在「A 城市商品增多」还是「现在 C 城市也要往 B 城市运输东西」这样的情况下都会出现「吞吐量不足」的问题。所以现在引入<strong>分区（Partition）</strong>的概念，类似“允许多修几条道”的方式对我们的主题完成了水平扩展。</p>
<h5 id="3-Broker与集群"><a href="#3-Broker与集群" class="headerlink" title="3. Broker与集群"></a>3. Broker与集群</h5><p>一个 Kafka <strong>服务器也称为 Broker</strong>，它接受生产者发送的消息并存入磁盘；Broker 同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。使用特定的机器硬件，一个 Broker 每秒可以处理成千上万的分区和百万量级的消息。</p>
<p>若干个 Broker 组成一个<strong>集群（Cluster）</strong>，其中集群内某个 Broker 会成为<strong>集群控制器</strong>（Cluster Controller），它负责管理集群，包括分配分区到 Broker、监控 Broker 故障等。在集群内，<strong>一个分区由一个 Broker 负责</strong>，这个 Broker 也称为这个分区的 <strong>Leader</strong>；当然一个分区可以被复制到多个 Broker 上来实现冗余，这样当存在 Broker 故障时可以将其分区重新分配到其他 Broker 来负责。下图是一个样例：</p>
<img src="assets/Broker和集群.png" alt="Broker和集群" style="zoom:50%;" />

<p>Kafka 的一个关键性质是<strong>日志保留（retention）</strong>，可以配置主题的消息保留策略，譬如只保留一段时间的日志或者只保留特定大小的日志。当超过这些限制时，老的消息会被删除。也可以针对某个主题单独设置消息过期策略，这样对于不同应用可以实现个性化。</p>
<h5 id="4-多集群"><a href="#4-多集群" class="headerlink" title="4. 多集群"></a>4. 多集群</h5><p>随着业务发展往往需要多集群，通常处于下面几个原因：</p>
<ul>
<li>基于数据的隔离。</li>
<li>基于安全的隔离。</li>
<li>多数据中心（容灾）。</li>
</ul>
<p>当构建多个数据中心时，往往需要实现消息互通。举个例子，假如用户修改了个人资料，那么后续的请求无论被哪个数据中心处理，这个更新需要反映出来。又或者多个数据中心的数据需要汇总到一个总控中心来做数据分析。</p>
<p>上面说的分区复制冗余机制只适用于同一个 Kafka 集群内部，对于多个 Kafka 集群消息同步可以使用 Kafka 提供的 MirrorMaker 工具。本质上来说，MirrorMaker 只是一个 Kafka 消费者和生产者，并使用一个队列连接起来而已。它从一个集群中消费消息，然后往另一个集群生产消息。</p>
<h4 id="Kafka架构模型"><a href="#Kafka架构模型" class="headerlink" title="Kafka架构模型"></a>Kafka架构模型</h4><img src="assets/nAcIiT.png" alt="nAcIiT.png" style="zoom:50%;" />

<ul>
<li><p><strong>Producer（生产者）</strong>：消息生产者。</p>
</li>
<li><p><strong>Consumer（消费者）</strong>：消息消费者。</p>
</li>
<li><p><strong>Consumer Group(CG)</strong>：<strong>消费者组</strong>，由多个 consumer 组成。消费者组内每个消费者负责消费<strong>不同分区 Partition</strong> 的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是<strong>逻辑上的一个订阅者</strong>。如果只有一个消费组，那么架构其实就是消息模型中的点对点模型，如果有多个消费组，那就对应了消息模型中的发布订阅模型。</p>
</li>
<li><p><strong>Broker</strong>：可以看作是一个独立的 Kafka <strong>服务器实例</strong>。多个 Kafka Broker 组成一个 Kafka Cluster。</p>
</li>
<li><p><strong>Topic（主题）</strong>：Producer 将消息发送到特定的<strong>主题</strong>，Consumer 通过订阅特定的 Topic(主题) 来消费消息。</p>
</li>
<li><p><strong>Partition（分区）</strong>：<strong>Partition</strong> 属于 Topic 的一部分。一个 <strong>Topic 可以有多个 Partition</strong>，并且同一 Topic 下的 Partition 可以<strong>分布在不同的 Broker 上</strong>，这也就表明一个 Topic 可以横跨多个 Broker 。Kafka 中的 <strong>Partition</strong> 实际上可以对应成为<strong>消息队列中的队列</strong>。分区主要使用来实现负载均衡。</p>
</li>
<li><p><strong>Replica</strong>：为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了<strong>副本机制</strong>，一个 topic 的每个分区都有若干个副本，副本包含一个 <strong>leader</strong> 和若干个 <strong>follower</strong>。</p>
</li>
<li><p><strong>Leader</strong>：每个分区<strong>多个副本的“主”</strong>，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。</p>
</li>
<li><p><strong>Follower</strong>：每个分区<strong>多个副本中的“从</strong>”，实时从 leader 中<strong>同步</strong>数据，保持和 leader 数据的同步。leader 发生故障时，选举一个 follower 会成为新的 leader。</p>
</li>
</ul>
<p>服务端(brokers)和客户端(producer、consumer)之间通信通过 <strong>TCP 协议</strong>来完成。</p>
<h4 id="Kafka与存储实现"><a href="#Kafka与存储实现" class="headerlink" title="Kafka与存储实现"></a>Kafka与存储实现</h4><h5 id="1-Kafka与文件系统"><a href="#1-Kafka与文件系统" class="headerlink" title="1. Kafka与文件系统"></a>1. Kafka与文件系统</h5><p>Kafka 的消息是<strong>存在于文件系统</strong>之上的。Kafka 高度依赖文件系统来存储和缓存消息，一般的人认为 “磁盘是缓慢的”，所以对这样的设计持有怀疑态度。实际上，磁盘比人们预想的快很多也慢很多，这取决于它们如何被使用；一个好的磁盘结构设计可以使之跟网络速度一样快。</p>
<p>现代的操作系统针对<strong>磁盘的读写</strong>已经做了一些优化方案来加快磁盘的访问速度。比如，<strong>预读</strong>会提前将一个比较大的磁盘快读入内存。<strong>后写</strong>会将很多小的逻辑写操作合并起来组合成一个大的物理写操作。并且，操作系统还会将主内存剩余的所有空闲内存空间都用作<strong>磁盘缓存</strong>，所有的磁盘读写操作都会经过统一的磁盘缓存（除了直接 I/O 会绕过磁盘缓存）。综合这几点优化特点，<strong>如果是针对磁盘的顺序访问，某些情况下它可能比随机的内存访问都要快，甚至可以和网络的速度相差无几</strong>。</p>
<p><strong>Topic 其实是逻辑上的概念，面向消费者和生产者，物理上存储的其实是 Partition</strong>，每一个 Partition 最终对应一个<strong>目录，里面存储所有的消息和索引文件</strong>。默认情况下，每一个 Topic 在创建时如果不指定 Partition 数量时只会创建 1 个 Partition。比如创建一个 Topic 名字为 test ，没有指定 Partition 的数量，那么会默认创建一个 <strong>test-0 的文件夹</strong>，这里的命名规则是：<strong><topic_name>-<partition_id></strong>。</p>
<img src="assets/kafka存在文件系统上.png" alt="主题（Topic）与分区（Partition）" style="zoom:47%;" />

<p>任何发布到 Partition 的消息都会被<strong>追加到 Partition 数据文件的尾部</strong>，这样的<strong>顺序写磁盘操作让 Kafka 的效率非常高</strong>（经验证，<strong>==顺序写磁盘==</strong>效率比随机写内存还要高，这是 Kafka 高吞吐率的一个很重要的保证）。</p>
<p>每一条消息被发送到 Broker 中，会<strong>根据 Partition 规则</strong>选择被存储到哪一个 Partition。如果 Partition 规则设置的合理，所有消息可以均匀分布到不同的 Partition中。</p>
<h5 id="2-底层存储细节"><a href="#2-底层存储细节" class="headerlink" title="2. 底层存储细节"></a>2. 底层存储细节</h5><p>假设现在 Kafka 集群<strong>只有一个 Broker</strong>，创建 2 个 Topic 名称分别为：「topic1」和「topic2」，Partition 数量分别为 1、2，那么的<strong>根目录</strong>下就会创建如下<strong>三个文件夹</strong>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">| --topic1-0</span><br><span class="line">| --topic2-0</span><br><span class="line">| --topic2-1</span><br></pre></td></tr></table></figure>

<p>在 Kafka 的<strong>文件存储</strong>中，<strong>同一个 Topic 下有多个不同的 Partition</strong>，每个 Partition 都为一个<strong>目录</strong>，而每一个目录又被平均分配成多个大小相等的 <strong>Segment File</strong> 中，Segment File 又由 <strong>index file 和 data file 组成</strong>，他们总是<strong>成对出现</strong>，后缀 “.index” 和 “.log” 分表表示 Segment <strong>索引文件和数据文件</strong>。<strong>索引文件中的元数据指向对应数据文件中</strong> message 的物理偏移地址。</p>
<img src="assets/image-20200728174836476.png" alt="image-20200728174836476" style="zoom:50%;" />

<p>现在假设设置每个 Segment 大小为 500 MB，并启动生产者向 topic1 中写入大量数据，topic1-0 文件夹中就会产生类似如下的一些文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">| --topic1-0 </span><br><span class="line">   | --00000000000000000000.index </span><br><span class="line">   | --00000000000000000000.log </span><br><span class="line">   | --00000000000000368769.index </span><br><span class="line">   | --00000000000000368769.log </span><br><span class="line">   | --00000000000000737337.index </span><br><span class="line">   | --00000000000000737337.log </span><br><span class="line">   | --00000000000001105814.index </span><br><span class="line">   | --00000000000001105814.log </span><br><span class="line">| --topic2-0 </span><br><span class="line">| --topic2-1</span><br></pre></td></tr></table></figure>

<p><strong>Segment 是 Kafka 文件存储的最小单位。</strong>Segment 文件命名规则：Partition 全局的第一个 Segment <strong>从 0</strong> 开始，后续每个 Segment 文件名为上一个 Segment 文件<strong>最后一条消息的 offset 值</strong>。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用 0 填充。如 00000000000000368769.index 和 00000000000000368769.log。</p>
<p>以上面的一对 Segment File 为例，说明一下<strong>索引文件和数据文件对应关系</strong>：</p>
<img src="assets/segment是kafka文件存储的最小单位.png" alt="索引文件和数据文件" style="zoom:77%;" />

<p>其中以索引文件中元数据 <strong>&lt;3, 497&gt;</strong> 为例，依次在数据文件中表示第 3 个 message（在全局 Partition 表示第 368769 + 3 = 368772 个 message）以及该消息的物理偏移地址为 497。</p>
<p>注意该 index 文件并不是从 0 开始，也不是每次递增 1 的，这是因为 Kafka 采取<strong>稀疏索引存储</strong>的方式，每隔一定字节的数据建立一条索引，它减少了索引文件大小，使得能够把 index 映射到内存，降低了查询时的磁盘 IO 开销，同时也并没有给查询带来太多的时间消耗。</p>
<p>因为其文件名为上一个 Segment 最后一条消息的 offset ，所以当需要查找一个指定 offset 的 message 时，通过在所有 segment 的文件名中进行<strong>二分查找</strong>就能找到它归属的 segment ，再在其 index 文件中找到其对应到文件上的物理位置，就能拿出该 message。</p>
<p>由于消息在 Partition 的 Segment 数据文件中是<strong>顺序读写</strong>的，且消息消费后不会删除（删除策略是针对过期的 Segment 文件），这种顺序磁盘 IO 存储设计师 Kafka 高性能很重要的原因。</p>
<blockquote>
<p>Kafka 是如何准确的知道 message 的偏移的呢？</p>
</blockquote>
<p>这是因为在 Kafka 定义了标准的数据存储结构，在 Partition 中的<strong>每一条 message 都包含了以下三个属性</strong>：</p>
<ul>
<li><p><strong>offset</strong>：表示 message 在当前 Partition 中的偏移量，是一个逻辑上的值，唯一确定了 Partition 中的一条 message，可以简单的认为是一个 id。</p>
</li>
<li><p><strong>MessageSize</strong>：表示 message 内容 data 的大小。</p>
</li>
<li><p><strong>data</strong>：message 的具体内容。</p>
</li>
</ul>
<h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><h5 id="1-写消息流程"><a href="#1-写消息流程" class="headerlink" title="1. 写消息流程"></a>1. 写消息流程</h5><p>流程如下：</p>
<ol>
<li>首先创建一个 <strong>ProducerRecord</strong>，这个对象需要包含消息的<strong>主题（topic）和值（value）</strong>，可以选择性<strong>指定一个键值（key）或者分区（partition）</strong>。</li>
<li>发送消息时，生产者会对键值和值<strong>序列化成字节数组</strong>，然后发送到<strong>分配器（partitioner）</strong>。</li>
<li>如果指定了<strong>分区</strong>，那么分配器返回该分区即可；否则，分配器将会<strong>基于键值</strong>来选择一个分区并返回。</li>
<li>选择完分区后，生产者知道了消息所属的主题和分区，它将这条记录添加到相同主题和分区的<strong>批量消息</strong>中，另一个线程负责发送这些批量消息到对应的 Kafka broker。</li>
<li>当 broker 接收到消息后，如果成功写入则<strong>返回一个包含消息的主题、分区及位移的 RecordMetadata 对象</strong>，否则返回异常。</li>
<li>生产者接收到结果后，对于异常可能会进行重试。</li>
</ol>
<h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><h5 id="1-消费者与消费组"><a href="#1-消费者与消费组" class="headerlink" title="1. 消费者与消费组"></a>1. 消费者与消费组</h5><p>如果生产者写入消息的速度比消费者读取的速度快，这样随着时间增长，<strong>消息堆积</strong>越来越严重。对于这种场景可以<strong>增加多个消费者</strong>来进行<strong>水平扩展</strong>。</p>
<p>Kafka 消费者是<strong>消费组</strong>的一部分，当<strong>多个消费者形成一个消费组</strong>来消费主题时，每个消费者会收到<strong>不同分区</strong>的消息。假设有一个 T1 主题，该主题有 4 个分区；同时有一个消费组 G1，这个消费组只有一个消费者 C1。那么消费者 C1 将会收到这 4 个分区的消息，如下所示：</p>
<img src="assets/image-20200728163144182.png" alt="image-20200728163144182" style="zoom:50%;" />

<p>如果增加新的消费者 C2 到消费组 G1，那么每个消费者将会分别收到<strong>两个分区</strong>的消息，如下所示：</p>
<img src="assets/image-20200728163249009.png" alt="image-20200728163249009" style="zoom:50%;" />

<p>如果增加到 4 个消费者，那么每个消费者将会分别收到<strong>一个分区</strong>的消息，如下所示：</p>
<img src="assets/image-20200728163330121.png" alt="image-20200728163330121" style="zoom:50%;" />

<p>但如果继续增加消费者到这个消费组，<strong>剩余的消费者将会空闲</strong>，不会收到任何消息：</p>
<img src="assets/image-20200728163404629.png" alt="image-20200728163404629" style="zoom:50%;" />

<p>总而言之，可以通过增加消费组的消费者来进行水平扩展提升消费能力。这也是为什么建议创建主题时使用比较多的分区数，这样可以在消费负载高的情况下增加消费者来提升性能。另外，消费者的数量<strong>不应该比分区数多</strong>，因为多出来的消费者是空闲的，没有任何帮助。</p>
<p><strong>Kafka 一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。</strong>换句话说，<strong>每个应用都可以读到全量的消息</strong>。为了使得每个应用都能读到全量消息，应用需要有<strong>不同的消费组</strong>。对于上面的例子，假如新增了一个新的消费组 G2，而这个消费组有<strong>两个消费者</strong>，那么会是这样的：</p>
<img src="assets/image-20200728163529141.png" alt="image-20200728163529141" style="zoom:50%;" />

<p>在这个场景中，消费组 G1 和消费组 G2 都能收到 T1 主题的<strong>全量消息</strong>，在逻辑意义上来说它们<strong>属于不同的应用</strong>。</p>
<p>总结：如果应用需要读取全量消息，那么请<strong>为该应用设置一个消费组</strong>；如果该应用消费能力不足，那么考虑在这个消费组里增加消费者。</p>
<h5 id="2-消费组与分区重平衡"><a href="#2-消费组与分区重平衡" class="headerlink" title="2. 消费组与分区重平衡"></a>2. 消费组与分区重平衡</h5><p>当<strong>新的消费者</strong>加入<strong>消费组</strong>，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；另外当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会<strong>分配给其他分区</strong>。这种现象称为<strong>重平衡（rebalance）</strong>。重平衡是 Kafka 一个很重要的性质，这个性质保证了高可用和水平扩展。<strong>不过也需要注意到，在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。</strong>而且将分区进行重平衡也会<strong>导致原来的消费者状态过期</strong>，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。后面我们会讨论如何安全的进行重平衡以及如何尽可能避免。</p>
<p>消费者通过<strong>定期发送心跳</strong>（hearbeat）到一个作为组协调者（group coordinator）的 broker 来保持在消费组内存活。这个 broker 不是固定的，每个消费组都可能不同。当消费者拉取消息或者提交时，便会发送心跳。</p>
<p>如果消费者超过一定时间没有发送心跳，那么它的<strong>会话（session）就会过期</strong>，组协调者会认为该消费者已经宕机，然后触发重平衡。可以看到，从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；通常情况下，可以进行<strong>优雅关闭</strong>，这样消费者会发送离开的消息到组协调者，这样组协调者可以<strong>立即进行重平衡</strong>而不需要等待会话过期。</p>
<p>在 0.10.1 版本，Kafka 对心跳机制进行了修改，将发送心跳与拉取消息进行分离，这样使得发送心跳的频率不受拉取的频率影响。另外更高版本的 Kafka 支持配置一个消费者多长时间不拉取消息但仍然保持存活，这个配置可以避免活锁（livelock）。<strong>活锁</strong>是指应用没有故障但是由于某些原因不能进一步消费。</p>
<h5 id="3-Partition与消费模型"><a href="#3-Partition与消费模型" class="headerlink" title="3. Partition与消费模型"></a>3. Partition与消费模型</h5><p>Kafka 中一个 topic 中的消息是<strong>被打散分配在多个 Partition(分区) 中存储的</strong>， Consumer Group 在消费时需要从不同的 Partition 获取消息，那最终如何重建出 Topic 中消息的<strong>顺序</strong>？</p>
<p>答案是：<strong>没有办法</strong>。<strong>Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况</strong>。</p>
<p>下一个问题是：Partition 中的<strong>消息可以被（不同的 Consumer Group）多次消费</strong>，那 Partition中被消费的消息是何时删除的？ Partition 又是如何知道一个 Consumer Group 当前消费的位置呢？</p>
<p>无论消息是否被消费，<strong>除非消息到期 Partition 从不删除消息</strong>。例如设置保留时间为 2 天，则消息发布 2 天内任何 Group 都可以消费，2 天后消息自动被删除。<br>Partition 会为<strong>每个 Consumer Group 保存一个偏移量</strong>，记录 <strong>Group 消费到的位置</strong>。 如下图：</p>
<img src="assets/image-20200728164036156.png" alt="image-20200728164036156" style="zoom:50%;" />

<h5 id="4-Kafka与pull模型"><a href="#4-Kafka与pull模型" class="headerlink" title="4. Kafka与pull模型"></a>4. Kafka与pull模型</h5><p>消费者应该向 Broker <strong>要数据（pull）</strong>还是 Broker 向消费者<strong>推送数据（push）</strong>？</p>
<p><strong>consumer 采用 pull（拉）模式从 broker 中读取数据。</strong></p>
<p>push 模式和 pull 模式各有优劣。</p>
<p><strong>push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的</strong>。push 模式的目标是尽可能以最快速度传递消息，但是这样很容易造成 Consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。<strong>而 pull 模式则可以根据 Consumer 的消费能力以适当的速率消费消息。</strong></p>
<p><strong>对于 Kafka 而言，pull 模式更合适。</strong>pull 模式<strong>可简化 broker 的设计</strong>，Consumer 可自主控制<strong>消费消息的速率</strong>，同时 Consumer 可以自己控制消费方式：即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p>
<h5 id="5-Kafka如何保证消息消费顺序"><a href="#5-Kafka如何保证消息消费顺序" class="headerlink" title="5. Kafka如何保证消息消费顺序"></a>5. Kafka如何保证消息消费顺序</h5><p>在使用消息队列的过程中经常有业务场景需要<strong>严格保证</strong>消息的消费顺序，比如同时发了 2 个消息，这 2 个消息对应的操作分别对应的数据库操作是：更改用户会员等级、根据会员等级计算订单价格。假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。</p>
<p>Kafka 中 <strong>Partition(分区)是真正保存消息的地方</strong>，发送的消息都被放在了这里。而的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且可以给 Topic 指定<strong>多个 Partition</strong>。</p>
<img src="assets/image-20200728170112324.png" alt="image-20200728170112324" style="zoom:50%;" />

<p>每次添加消息到 Partition(分区) 的时候都会采用<strong>尾加法</strong>，如上图所示。Kafka 只能保证<strong>一个 Partition(分区) 中的消息有序</strong>，而<strong>不能保证 Topic(主题) 中所有 Partition(分区) 的全局有序性</strong>。</p>
<p>消息在被追加到 <strong>Partition</strong>(分区) 的时候都会<strong>分配一个特定的偏移量</strong>（offset）。Kafka <strong>通过偏移量（offset）来保证消息在分区内的顺序性</strong>。</p>
<p>所以就有一种很简单的保证消息消费顺序的方法：<strong>1 个 Topic 只对应一个 Partition</strong>。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。</p>
<p>还有一种方法，Kafka 中发送一条消息的时候，可以指定 <strong>topic，partition，key，data</strong>（数据） 4 个参数。如果发送消息的时候指定了 Partition 的话，所有消息都<strong>会被发送到指定的 Partition</strong>。并且同一个 key 的消息可以<strong>保证只发送到同一个 partition</strong>，这个可以采用表/对象的 id 来作为 key 。</p>
<p>总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法：</p>
<ol>
<li>1 个 Topic 只对应一个 Partition。</li>
<li><strong>发送消息的时候通过参数发送到指定的 key/Partition</strong>（推荐）。</li>
</ol>
<h5 id="6-如何保证消息不丢失-可靠性"><a href="#6-如何保证消息不丢失-可靠性" class="headerlink" title="6. 如何保证消息不丢失/可靠性"></a>6. 如何保证消息不丢失/可靠性</h5><h6 id="1-生产者丢失消息"><a href="#1-生产者丢失消息" class="headerlink" title="(1) 生产者丢失消息"></a>(1) 生产者丢失消息</h6><p>生产者(Producer) 调用 send 方法发送消息之后，<strong>消息可能因为网络问题</strong>并没有发送过去。 所以不能默认在调用 send 方法发送消息之后消息消息发送成功了。为了确定消息是发送成功，<strong>需要判断消息发送的结果</strong>。但是要注意的是  Kafka 生产者(Producer) 使用  send 方法发送消息实际上是<strong>异步操作</strong>，可以通过 get()方法获取调用结果，但是这样也让它变为了<strong>同步操作</strong>，示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SendResult&lt;String, Object&gt; sendResult = kafkaTemplate.send(topic, o).get();</span><br><span class="line"><span class="keyword">if</span> (sendResult.getRecordMetadata() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    logger.info(<span class="string">"生产者成功发送消息到"</span> + sendResult.getProducerRecord().topic() </span><br><span class="line">                + <span class="string">"-&gt; "</span> + sendResult.getProducerRecord().value().toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是一般<strong>不推荐这么做</strong>！可以采用为其<strong>添加回调函数</strong>的形式，示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, o);</span><br><span class="line">future.addCallback(result </span><br><span class="line">                   -&gt; logger.info(<span class="string">"生产者成功发送消息到topic:&#123;&#125; partition:&#123;&#125;的消息"</span>, </span><br><span class="line">                                  result.getRecordMetadata().topic(), </span><br><span class="line">                                  result.getRecordMetadata().partition()),</span><br><span class="line">                   ex </span><br><span class="line">                   -&gt; logger.error(<span class="string">"生产者发送消失败，原因：&#123;&#125;"</span>, </span><br><span class="line">                                   ex.getMessage()));</span><br></pre></td></tr></table></figure>

<p>如果消息发送失败的话，检查失败的原因之后<strong>重新发送</strong>即可！</p>
<p>另外这里推荐为 Producer 的 <strong>retries（重试次数）设置一个比较合理的值</strong>，一般是 <strong>3</strong>，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够<strong>自动重试消息发送</strong>，避免消息丢失。另外，建议还要设置<strong>重试间隔</strong>，因为间隔太小的话重试的效果就不明显了。</p>
<h6 id="2-消费者丢失消息"><a href="#2-消费者丢失消息" class="headerlink" title="(2) 消费者丢失消息"></a>(2) 消费者丢失消息</h6><p>消息在被追加到 Partition (分区)的时候都会<strong>分配一个特定的偏移量</strong>（offset）。偏移量（offset) 表示 Consumer 当前消费到的 Partition(分区) 的所在的位置。Kafka 通过<strong>偏移量（offset）可以保证消息在分区内的顺序性</strong>。</p>
<img src="assets/image-20200728164036156.png" alt="image-20200728164036156" style="zoom:50%;" />

<p>当消费者<strong>拉取</strong>到了分区的某个消息之后，<strong>消费者会自动提交 offset</strong>。自动提交的话会有一个问题，如果当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，<strong>消息实际上并没有被消费，但是 offset 却被自动提交了</strong>。</p>
<p><strong>解决办法也比较粗暴，关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。</strong> 但是这样会带来消息被<strong>重新消费</strong>的问题。比如刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。</p>
<h6 id="3-Kafka弄丢消息"><a href="#3-Kafka弄丢消息" class="headerlink" title="(3) Kafka弄丢消息"></a>(3) Kafka弄丢消息</h6><p>Kafka 为分区（Partition）引入了多副本（Replica）机制。试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。</p>
<p><strong>(1) 设置 acks = all</strong>：解决办法就是设置  <strong>acks = all</strong>。acks 是 Kafka 生产者(Producer)  很重要的一个参数。acks 的默认值即为 1，代表消息被 leader 副本接收之后就算被成功发送。当配置 <strong>acks = all</strong> 代表则<strong>所有副本都要接收到该消息</strong>之后该消息才算真正成功被发送。</p>
<p><strong>(2) 设置 replication.factor &gt;= 3</strong>：为了保证 leader 副本能有 follower 副本能同步消息，一般会为 topic 设置 <strong>replication.factor &gt;= 3</strong>。这样就可以<strong>保证每个分区(partition) 至少有 3 个副本</strong>。虽然造成了数据冗余，但是带来了数据的安全性。</p>
<p><strong>(3) 设置 min.insync.replicas &gt; 1</strong>：一般情况下还需要设置 <strong>min.insync.replicas&gt; 1</strong> ，这样配置代表消息至少要被写入到 <strong>2 个副本</strong>才算是被成功发送。<strong>min.insync.replicas</strong> 的默认值为 1 ，在实际生产中应尽量避免默认值 1。但是，为了保证整个 Kafka 服务的高可用性，需要确保 <strong>replication.factor &gt; min.insync.replicas</strong> 。为什么呢？设想一下加入两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 <strong>replication.factor = min.insync.replicas + 1</strong>。</p>
<p><strong>(4) 设置 unclean.leader.election.enable = false</strong>：Kafka 0.11.0.0 版本开始 <strong>unclean.leader.election.enable</strong> 参数的默认值由原来的 true 改为 false。消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的<strong>消息同步情况不一样</strong>，当配置了 <strong>unclean.leader.election.enable = false</strong>  的话，当 leader 副本发生故障时就不会从  follower 副本中和 leader 同步程度达不到要求的副本中选择出  leader ，这样降低了消息丢失的可能性。</p>
<h5 id="7-如何保证消息不被重复消费"><a href="#7-如何保证消息不被重复消费" class="headerlink" title="7. 如何保证消息不被重复消费"></a>7. 如何保证消息不被重复消费</h5><p>如何保证消息<strong>不被重复消费</strong>？换句话说就是如何保证消息消费的<strong>幂等性</strong>？</p>
<h6 id="1-重复消费问题"><a href="#1-重复消费问题" class="headerlink" title="(1) 重复消费问题"></a>(1) 重复消费问题</h6><p><strong>每个消息</strong>被写进去后都有一个 <strong>offset</strong>，代表其序号，然后 consumer 消费该数据之后，隔一段时间，会把<strong>自己消费过的消息的 offset 提交一下</strong>，代表已经消费过了。下次要是重启，就会继续从上次<strong>消费到的 offset</strong> 来继续消费。</p>
<p>但是如果直接 kill 消费者进程然后再重启，就导致 consumer 有些消息处理了，但是<strong>没来得及提交 offset</strong>。等重启之后，少数消息就会<strong>再次消费一次</strong>。</p>
<p>其他 MQ 也会有这种重复消费的问题，那么针对这种问题需要从业务角度，考虑它的幂等性。</p>
<p>举个例子，当消费一条消息时就往数据库插入一条数据。如何<strong>保证重复消费</strong>也插入一条数据呢？那就需要从<strong>幂等性</strong>角度考虑了。</p>
<h6 id="2-保证幂等性"><a href="#2-保证幂等性" class="headerlink" title="(2) 保证幂等性"></a>(2) 保证幂等性</h6><p><strong>怎么保证消息队列消费的幂等性？</strong>需要结合业务来思考，比如下面的例子：</p>
<ul>
<li>比如某个数据要<strong>写数据库</strong>，先根据<strong>主键查一下</strong>，如果数据有了，就别插入了而是 <strong>update</strong> 一下。</li>
<li>如果是<strong>写 Redis</strong>，那没问题，反正每次都是 set，<strong>天然幂等性</strong>。</li>
<li>对于消息可以<strong>建个表</strong>（专门存储消息消费记录）。生产者发送消息前<strong>判断库中是否有记录</strong>（有记录说明已发送），没有记录，先入库，状态为<strong>待消费</strong>，然后发送消息并把主键 id 带上。消费者接收消息时通过主键 ID 查询记录表，判断消息状态是否已消费。若没消费过，则处理消息，处理完后，更新消息记录的状态为已消费。</li>
</ul>
<h5 id="8-offset的维护"><a href="#8-offset的维护" class="headerlink" title="8. offset的维护"></a>8. offset的维护</h5><p>由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 <strong>offset</strong>，以便故障恢复后继续消费。</p>
<p><strong>group + topic + partition（GTP） 才能确定一个 offset！</strong></p>
<p><img src="assets/1567511669385.png" alt="1567511669385"></p>
<p>Kafka 0.9 版本之前，consumer 默认<strong>将 offset 保存在 Zookeeper 中</strong>，从 0.9 版本开始，consumer 默认<strong>将 offset 保存在 Kafka 本地一个内置的 topic 中</strong>，该 topic 为 <strong>__consumer_offsets</strong>（此时消费者对于 offset 相当于生产者）。</p>
<p>1）修改配置文件 consumer.properties。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">exclude.internal.topics</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure>

<p>2）读取 offset。</p>
<ul>
<li>0.11.0.0 之前版本：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafkabin/kafka--consoleconsole--consumer.sh consumer.sh ----topic __consumer_offsets topic __consumer_offsets ----zookeeper zookeeper hadoophadoop102102:2181 :2181 ----formatter formatter</span><br><span class="line"></span><br><span class="line">"kafka.coordinator.GroupMetadataManager"kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter" $OffsetsMessageFormatter" ----consumer.config config/consumer.properties consumer.config config/consumer.properties ----fromfrom--beginningbeginning</span><br></pre></td></tr></table></figure>

<ul>
<li>0.11.0.0 之后版本(含)：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafkabin/kafka--consoleconsole--consumer.sh consumer.sh ----topic __consumer_offsets topic __consumer_offsets ----zookeeper zookeeper hadoophadoop102102:2181 :2181 ----formatter formatter</span><br><span class="line"></span><br><span class="line">"kafka.coordinator.group.GroupMetadataManager"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageForm$OffsetsMessageFormatter" atter" ----consumer.config config/consumer.propertiesconsumer.config config/consumer.properties ----fromfrom--beginningbeginning</span><br></pre></td></tr></table></figure>

<p>同一个消费者组中的消费者， 同一时刻只能有一个消费者消费。</p>
<h4 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h4><h5 id="1-概述-1"><a href="#1-概述-1" class="headerlink" title="1. 概述"></a>1. 概述</h5><p>Kafka 中的可靠性保证有如下四点：</p>
<ul>
<li>对于<strong>一个分区</strong>来说，它的<strong>消息是有序</strong>的。如果一个生产者向一个分区先写入消息 A，然后写入消息 B，那么消费者会先读取消息 A 再读取消息 B。</li>
<li>当消息写入所有 <strong>in-sync 状态的副本</strong>后，消息才会认为<strong>已提交（committed）</strong>。这里的写入有可能只是写入到文件系统的<strong>缓存</strong>，不一定刷新到磁盘。生产者可以等待不同时机的确认，比如等待分区主副本<strong>写入即返回</strong>，或者等待所有 in-sync 状态副本写入才返回。</li>
<li>一旦消息已提交，那么<strong>只要有一个副本存活，数据不会丢失</strong>。</li>
<li>消费者只能<strong>读取到已提交</strong>的消息。</li>
</ul>
<p>可靠性不是无偿的，它与系统可用性、吞吐量、延迟和硬件价格息息相关，得此失彼。因此往往需要做权衡，一味的追求可靠性并不实际。</p>
<h5 id="2-多副本机制"><a href="#2-多副本机制" class="headerlink" title="2. 多副本机制"></a>2. 多副本机制</h5><p><strong>Kafka</strong> 为分区（Partition）引入了<strong>多副本（Replica）机制</strong>。分区（Partition）中的多个副本之间会有一个 leader ，其他副本称为 <strong>follower</strong>。发送的消息会<strong>被发送到 leader 副本</strong>，然后 follower 副本才能从 leader 副本中<strong>拉取消息进行同步</strong>。</p>
<p>生产者和消费者<strong>只与 leader 副本交互</strong>。其他副本只是 leader 副本的<strong>拷贝</strong>，它们的存在只是为了<strong>保证消息存储的安全性</strong>。当 leader 副本发生故障时会从 follower 中<strong>选举出一个 leader</strong>，但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。</p>
<p><strong>多副本机制的好处</strong>：</p>
<ol>
<li>Kafka 通过给特定 Topic 指定多个 Partition，而各个 Partition 可以<strong>分布在不同的 Broker 上</strong>，这样便能提供比较好的并发能力（<strong>负载均衡</strong>）。</li>
<li>Partition 可以指定对应的 Replica 数，这也极大地提高了消息存储的<strong>安全性</strong>，提高了容灾能力，不过也相应的增加了所需要的存储空间。</li>
</ol>
<h5 id="3-副本数据同步策略"><a href="#3-副本数据同步策略" class="headerlink" title="3. 副本数据同步策略"></a>3. 副本数据同步策略</h5><table>
<thead>
<tr>
<th align="center"><strong>方案</strong></th>
<th align="center"><strong>优点</strong></th>
<th align="center"><strong>缺点</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>半数以上完成同步，就发送 ack</strong></td>
<td align="center">延迟低</td>
<td align="center">选举新的 leader 时，容忍 n 台节点的故障，需要 2n+1 个副本</td>
</tr>
<tr>
<td align="center"><strong>全部完成同步，才发送 ack</strong></td>
<td align="center">选举新的 leader 时，容忍 n 台节点的故障，需要 n+1 个副本</td>
<td align="center">延迟高</td>
</tr>
</tbody></table>
<p>Kafka 选择了第二种方案，原因如下：</p>
<ol>
<li>同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</li>
<li>虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小（同一网络环境下的传输）。</li>
</ol>
<h4 id="Kafka与Zookeeper"><a href="#Kafka与Zookeeper" class="headerlink" title="Kafka与Zookeeper"></a>Kafka与Zookeeper</h4><p>ZooKeeper 主要为 Kafka 提供<strong>元数据的管理</strong>的功能。</p>
<ol>
<li><strong>Broker 注册</strong>：在 Zookeeper 上会有一个专门<strong>用来进行 Broker 服务器列表记录</strong>的节点。每个 Broker 在启动时，都会到 Zookeeper 上进行<strong>注册</strong>，即到 <strong>/brokers/ids</strong> 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去。</li>
<li><strong>Topic 注册</strong>：在 Kafka 中，同一个Topic 的消息会被<strong>分成多个分区并将其分布在多个 Broker 上</strong>，这些分区信息及与 Broker 的<strong>对应关系也都是由 Zookeeper 在维护</strong>。比如创建了一个名字为 <strong>my-topic</strong> 的主题并且它有两个分区，对应到 zookeeper 中会创建这些文件夹：<strong>/brokers/topics/my-topic/Partitions/0、/brokers/topics/my-topic/Partitions/1</strong>。</li>
<li><strong>负载均衡</strong>：Kafka 通过给特定 Topic 指定多个 Partition，而各个 Partition 可以分布在不同的 Broker 上，这样便能提供比较好的并发能力。 对于同一个 Topic 的不同 Partition，Kafka 会<strong>尽力将这些 Partition 分布到不同的 Broker 服务器上</strong>。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现<strong>动态负载均衡</strong>。</li>
</ol>
<p>以下为 partition 的 leader 选举过程：</p>
<img src="assets/kafka-zk.png" style="zoom:55%;" />

<h4 id="Kafka高效读写数据"><a href="#Kafka高效读写数据" class="headerlink" title="Kafka高效读写数据"></a>Kafka高效读写数据</h4><h5 id="1-顺序写磁盘"><a href="#1-顺序写磁盘" class="headerlink" title="1. 顺序写磁盘"></a>1. 顺序写磁盘</h5><p>Kafka 的 Producer 生产数据，要<strong>写入到 log 文件</strong>中，写的过程是一直<strong>追加到文件末端，为顺序写</strong>。官网有数据表明，同样的磁盘，顺序写能到到 <strong>600M/s</strong>，而随机写只有 <strong>100k/s</strong>。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。所以千万别以为写硬盘都很慢。</p>
<h5 id="2-零拷贝技术"><a href="#2-零拷贝技术" class="headerlink" title="2. 零拷贝技术"></a>2. 零拷贝技术</h5><p>Kafka 的消息会有<strong>多个订阅者</strong>，生产者发布的消息会被不同的消费者多次消费，为了优化这个流程，Kafka 使用了<strong>“零拷贝技术”</strong>。</p>
<img src="assets/1567513386020.png" alt="1567513386020" style="zoom:60%;" />

<img src="assets/1567513364566.png" alt="1567513364566" style="zoom:60%;" />

<p>“零拷贝技术” 只用将<strong>磁盘文件的数据复制到页面缓存中一次</strong>，然后将数据<strong>从页面缓存直接发送到网络中</strong>（发送给不同的订阅者时，都可以使用同一个页面缓存），避免了重复复制操作。</p>
<p>如果有 10 个消费者，传统方式下，数据复制次数为 4 * 10=40 次，而使用“零拷贝技术”只需要 1+10=11 次，一次为从磁盘复制到页面缓存，10 次表示 10 个消费者各自读取一次页面缓存。</p>
<h4 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h4><p>Kafka 从 0.11 版本开始引入了<strong>事务</strong>。事务可以保证 Kafka 在 <strong>Exactly Once 语义</strong>的基础上，生产和消费可以<strong>跨分区和会话，要么全部成功，要么全部失败</strong>。<strong>注意</strong>：这里的事务主要谈的是<strong>生产者</strong>（Producer）的事务。</p>
<h5 id="1-Producer事务"><a href="#1-Producer事务" class="headerlink" title="1. Producer事务"></a>1. Producer事务</h5><p>为了实现跨分区跨会话的事务，需要引入一个<strong>全局唯一的 Transaction ID</strong>（<strong>一定是客户端给的</strong>），并将 Producer 获得的 PID 和 Transaction ID 绑定。这样当 Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。 </p>
<p>为了管理 Transaction，Kafka 引入了一个新的组件 <strong>Transaction Coordinator</strong>。Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p>
<h5 id="2-Consumer事务"><a href="#2-Consumer事务" class="headerlink" title="2. Consumer事务"></a>2. Consumer事务</h5><p>上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对较弱，尤其时无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>
<h4 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h4><h5 id="1-消息发送流程"><a href="#1-消息发送流程" class="headerlink" title="1. 消息发送流程"></a>1. 消息发送流程</h5><p>Producer 发送消息采用的是<strong>异步发送</strong>的方式。消息发送的过程涉及两个线程：<strong>main 线程和 Sender 线程</strong>，以及一个线程共享变量：<strong>RecordAccumulator</strong>（接收器）。</p>
<p><strong>main 线程将消息发送给 RecordAccumulator，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka broker。</strong></p>
<img src="assets/kafka-produce.png" style="zoom:60%;" />

<p>相关参数：</p>
<ul>
<li><strong>batch.size</strong>：只有数据积累到 batch.size 之后，sender 才会发送数据。</li>
<li><strong>linger.ms</strong>：如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。</li>
</ul>
<h5 id="2-异步发送API"><a href="#2-异步发送API" class="headerlink" title="2. 异步发送API"></a>2. 异步发送API</h5><ul>
<li><strong>KafkaProducer</strong>：需要创建一个<strong>生产者对象</strong>，用来发送数据。</li>
<li><strong>ProducerConfig</strong>：获取所需的一系列配置参数。</li>
<li><strong>ProducerRecord</strong>：每条数据都要封装成一个 <strong>ProducerRecord</strong> 对象。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="1-不带回调函数的异步（AsyncProducer）"><a href="#1-不带回调函数的异步（AsyncProducer）" class="headerlink" title="(1) 不带回调函数的异步（AsyncProducer）"></a>(1) 不带回调函数的异步（AsyncProducer）</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 不带回调函数的异步Producer API</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,</span><br><span class="line">                  <span class="string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                  StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                  StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">1</span>);</span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 配置拦截器</span></span><br><span class="line">        <span class="comment">// 通过配置创建KafkaProducer对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>, <span class="string">"message"</span> + i);</span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-带回调函数的异步（CallbackProducer）"><a href="#2-带回调函数的异步（CallbackProducer）" class="headerlink" title="(2) 带回调函数的异步（CallbackProducer）"></a>(2) 带回调函数的异步（CallbackProducer）</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 带回调函数的异步Producer API</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CallbackProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,</span><br><span class="line">                  <span class="string">"192.168.72.133:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                  StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                  StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">1</span>);</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>, <span class="string">"message"</span> + i);</span><br><span class="line">            producer.send(record, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="keyword">null</span>)</span><br><span class="line">                        System.out.println(<span class="string">"success:"</span> + recordMetadata.topic() +</span><br><span class="line">                                           <span class="string">"-"</span> + recordMetadata.partition() +</span><br><span class="line">                                           <span class="string">"-"</span> + recordMetadata.offset());</span><br><span class="line">                    <span class="keyword">else</span> e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;); </span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-同步发送API"><a href="#3-同步发送API" class="headerlink" title="3. 同步发送API"></a>3. 同步发送API</h5><h6 id="1-同步发送（SyncProducer）"><a href="#1-同步发送（SyncProducer）" class="headerlink" title="(1) 同步发送（SyncProducer）"></a>(1) 同步发送（SyncProducer）</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 同步 Producer API</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 创建properties对象用于存放配置</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 添加配置</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"hadoop101:9092,hadoop102:9092,hadoop103:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">1</span>); <span class="comment">// 重试次数</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">500</span>);</span><br><span class="line">        <span class="comment">// 通过已有配置创建kafkaProducer对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="comment">// 循环调用 send 方法不断发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"first"</span>, <span class="string">"message"</span> + i);</span><br><span class="line">            <span class="comment">// 通过get()方法实现同步效果</span></span><br><span class="line">            RecordMetadata metadata = producer.send(record).get();</span><br><span class="line">            <span class="keyword">if</span> (metadata != <span class="keyword">null</span>)</span><br><span class="line">                System.out.println(<span class="string">"success:"</span> + metadata.topic() + <span class="string">"-"</span> +</span><br><span class="line">                        metadata.partition() + <span class="string">"-"</span> + metadata.offset());</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close(); <span class="comment">// 关闭生产者对象</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h4><p>Consumer 消费数据时的可靠性是很容易保证的，因为数据在 Kafka 中是<strong>持久化</strong>的，故一般不用担心数据丢失问题。</p>
<p>而由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从<strong>故障前的位置的继续消费</strong>，所以 consumer 需要<strong>实时记录自己消费到了哪个 offset</strong>，以便故障恢复后继续消费。所以 offset 的维护是 Consumer 消费数据是必须考虑的问题。</p>
<h5 id="1-自动提交offset"><a href="#1-自动提交offset" class="headerlink" title="1. 自动提交offset"></a>1. 自动提交offset</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据。</li>
<li><strong>ConsumerConfig</strong>：获取所需的一系列配置参数。</li>
<li><strong>ConsuemrRecord</strong>：每条数据都要封装成一个 ConsumerRecord 对象。</li>
</ul>
<p>为了能专注于自己的业务逻辑，Kafka 提供了<strong>自动提交 offset</strong> 的功能。自动提交 offset 的相关参数：</p>
<ul>
<li><strong>enable.auto.commit</strong>：是否开启自动提交 offset 功能。</li>
<li><strong>auto.commit.interval.ms</strong>：自动提交 offset 的时间间隔。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自动提交 offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AutoCommitOffset</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,</span><br><span class="line">                  <span class="string">"hadoop101:9092,hadoop102:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                  StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                  StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"tian"</span>); <span class="comment">// groupid</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">true</span>); <span class="comment">// 自动提交</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>)); <span class="comment">// 添加需要消费的 topic</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    System.out.println(record.value());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            consumer.close();<span class="comment">// 在死循环中无法调用 close 方法，所以需要使用 finally</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是自动提交可能引起<strong>消息消费失败</strong>的问题。</p>
<h5 id="2-手动提交offset"><a href="#2-手动提交offset" class="headerlink" title="2. 手动提交offset"></a>2. 手动提交offset</h5><p>虽然自动提交 offset 十分简介便利，但由于其是<strong>基于时间</strong>提交的，开发人员难以把握 offset 提交的时机。因此 Kafka 还提供了手动提交 offset 的 API。<br>手动提交 offset 的方法有两种: 分别是 <strong>commitSync</strong>（<strong>同步提交</strong>）和 <strong>commitAsync</strong>（<strong>异步提交</strong>）。两者的相同点是，都会将<strong>本次 poll 的一批数据最高的偏移量提交</strong>；不同点是，commitSync <strong>阻塞当前线程</strong>，一直到提交成功，并且会自动失败充实（由不可控因素导致，也会出现提交失败）；而 commitAsync 则没有失败重试机制，故有可能提交失败。</p>
<h6 id="1-同步提交commitSync-offset"><a href="#1-同步提交commitSync-offset" class="headerlink" title="(1) 同步提交commitSync offset"></a>(1) 同步提交commitSync offset</h6><p>由于同步提交 offset 有失败重试机制，<strong>故更加可靠</strong>，以下为同步提交 offset 的示例</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 同步手动提交 offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomComsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);<span class="comment">//Kafka 集群</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);<span class="comment">// 消费者组，只要 group.id 相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);<span class="comment">// 关闭自动提交 offset</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>));<span class="comment">// 消费者订阅主题</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 消费者拉取数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            consumer.commitSync();<span class="comment">// 同步提交，当前线程会阻塞直到offset提交成功</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-异步提交commitAsync-offset"><a href="#2-异步提交commitAsync-offset" class="headerlink" title="(2) 异步提交commitAsync offset"></a>(2) 异步提交commitAsync offset</h6><p>虽然同步提交 offset 更可靠一些，但是由于其会<strong>阻塞当前线程</strong>，直到提交成功。因此<strong>吞吐量会受到很大的影响</strong>，因此更多的情况下，会选用异步提交 offset 的方式。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 异步手动提交 offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncManualCommitOffset</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"hadoop101:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"tian"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.println(<span class="string">"offset:"</span> + record.offset() +</span><br><span class="line">                        <span class="string">"key:"</span> + record.key() + <span class="string">"value"</span> + record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; map, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (e != <span class="keyword">null</span>)</span><br><span class="line">                        System.out.println(<span class="string">"commit failed for"</span> + map);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);<span class="comment">// 异步提交</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="3-数据漏消费和重复消费分析"><a href="#3-数据漏消费和重复消费分析" class="headerlink" title="(3) 数据漏消费和重复消费分析"></a>(3) 数据漏消费和重复消费分析</h6><p>无论是同步提交还是异步提交 offset，都<strong>有可能会造成数据的漏消费或者重复消费</strong>。<strong>先提交 offset 后消费，有可能造成数据的漏消费；而先消费后提交 offset，有可能会造成数据的重复消费。</strong></p>
<img src="assets/kafka-offset.png" style="zoom:60%;" />

<h5 id="3-自定义存储offset"><a href="#3-自定义存储offset" class="headerlink" title="3. 自定义存储offset"></a>3. 自定义存储offset</h5><p>Kafka 0.9 版本之前，offset 存储在 <strong>zookeeper</strong>，0.9 版本之后，默认将 offset 存储在 Kafka 的一个<strong>内置的 topic 中</strong>。除此之外，Kafka 还可以选择<strong>自定义存储 offset</strong>。</p>
<p>offset 的维护是相当繁琐的，因为需要考虑到消费者的 <strong>Rebalance</strong>。当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会<strong>触发到分区的重新分配，重新分配的过程叫做 Rebalance。</strong></p>
<p>消费者发生 Rebalance 之后，每个<strong>消费者消费的分区就会发生变化</strong>。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区<strong>最近提交的 offset 位置</strong>继续消费。</p>
<p>要实现自定义存储 offset，需要借助 <strong>ConsumerRebalanceListener</strong>，以下为示例代码，其中提交和获取 offset 的方法，需要根据所选的 offset 存储系统自行实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义存储offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;TopicPartition, Long&gt; currentOffset = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);<span class="comment">// Kafka集群</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);<span class="comment">// 消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);<span class="comment">// 关闭自动提交offset</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        <span class="comment">// 消费者订阅主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"first"</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 该方法会在Rebalance之前调用</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                commitOffset(currentOffset);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 该方法会在Rebalance之后调用</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                currentOffset.clear();</span><br><span class="line">                <span class="keyword">for</span> (TopicPartition partition : partitions) &#123;</span><br><span class="line">                    <span class="comment">// 定位到最近提交的offset位置继续消费</span></span><br><span class="line">                    consumer.seek(partition, getOffset(partition));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 消费者拉取数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                currentOffset.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), record.offset());</span><br><span class="line">            &#125;</span><br><span class="line">            commitOffset(currentOffset); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取某分区的最新offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOffset</span><span class="params">(TopicPartition partition)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交该消费者所有分区的offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">commitOffset</span><span class="params">(Map&lt;TopicPartition, Long&gt; currentOffset)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="Spring整合Kafka"><a href="#Spring整合Kafka" class="headerlink" title="Spring整合Kafka"></a>Spring整合Kafka</h4><h5 id="1-Producer"><a href="#1-Producer" class="headerlink" title="1. Producer"></a>1. Producer</h5><h6 id="1-KafkaProducer"><a href="#1-KafkaProducer" class="headerlink" title="(1) KafkaProducer"></a>(1) KafkaProducer</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(KafkaProducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;app.topic.foo&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String topic;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"demo"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">send</span><span class="params">(@RequestParam String msg)</span></span>&#123;</span><br><span class="line">		<span class="comment">// LOG.info("sending message='&#123;&#125;' to topic='&#123;&#125;'", message, topic);</span></span><br><span class="line">        kafkaTemplate.send(topic,<span class="string">"key"</span>, msg);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"send success"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-KafkaProducerConfig"><a href="#2-KafkaProducerConfig" class="headerlink" title="(2) KafkaProducerConfig"></a>(2) KafkaProducerConfig</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.DefaultKafkaProducerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.ProducerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSenderConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.kafka.bootstrap-servers&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String bootstrapServers;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">producerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerFactory&lt;String, String&gt; <span class="title">producerFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DefaultKafkaProducerFactory&lt;&gt;(producerConfigs());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> KafkaTemplate&lt;String, String&gt; <span class="title">kafkaTemplate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> KafkaTemplate&lt;&gt;(producerFactory());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-Consumer"><a href="#2-Consumer" class="headerlink" title="2. Consumer"></a>2. Consumer</h5><h6 id="1-KafkaConsumer"><a href="#1-KafkaConsumer" class="headerlink" title="(1) KafkaConsumer"></a>(1) KafkaConsumer</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.Consumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.support.Acknowledgment;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Service;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(KafkaConsumer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(topics = <span class="string">"$&#123;app.topic.foo&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack, Consumer&lt;?, ?&gt; consumer)</span> </span>&#123;</span><br><span class="line">        LOG.warn(<span class="string">"topic:&#123;&#125;,key: &#123;&#125;,partition:&#123;&#125;, value: &#123;&#125;, record: &#123;&#125;"</span>,record.topic(), record.key(),record.partition(), record.value(), record);</span><br><span class="line">        <span class="keyword">if</span> (record.topic().equalsIgnoreCase(<span class="string">"test"</span>))&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">"提交 offset "</span>);</span><br><span class="line">        consumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-KafkaConsumerConfig"><a href="#2-KafkaConsumerConfig" class="headerlink" title="(2) KafkaConsumerConfig"></a>(2) KafkaConsumerConfig</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.EnableKafka;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.KafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.ConsumerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.DefaultKafkaConsumerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.listener.ConcurrentMessageListenerContainer;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.listener.ContainerProperties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="meta">@EnableKafka</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.kafka.bootstrap-servers&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String bootstrapServers;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">consumerConfigs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"foo"</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerFactory&lt;String, String&gt; <span class="title">consumerFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory() &#123;</span><br><span class="line">        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = <span class="keyword">new</span> ConcurrentKafkaListenerContainerFactory&lt;&gt;();</span><br><span class="line">        factory.setConcurrency(<span class="number">3</span>);</span><br><span class="line">        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);</span><br><span class="line">        factory.setConsumerFactory(consumerFactory());</span><br><span class="line">        <span class="keyword">return</span> factory;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-配置文件"><a href="#3-配置文件" class="headerlink" title="3. 配置文件"></a>3. 配置文件</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">9090</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">:9092</span></span><br><span class="line"><span class="attr">app:</span></span><br><span class="line">  <span class="attr">topic:</span></span><br><span class="line">    <span class="attr">foo:</span> <span class="string">frankfeekr</span></span><br></pre></td></tr></table></figure>

<h5 id="4-pom依赖"><a href="#4-pom依赖" class="headerlink" title="4. pom依赖"></a>4. pom依赖</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.7.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>/&gt;</span> <span class="comment">&lt;!-- lookup parent from repository --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.frankfeekr<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springboot-kafka-sample<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>springboot-kafka-sample<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Demo project for Spring Boot<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.7.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>







<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul>
<li><p><a href="https://www.infoq.cn/article/kafka-analysis-part-1" target="_blank" rel="noopener">https://www.infoq.cn/article/kafka-analysis-part-1</a> - Kafka 设计解析（一）：Kafka 背景及架构介绍</p>
</li>
<li><p><a href="http://www.dengshenyu.com/分布式系统/2017/11/06/kafka-Meet-Kafka.html" target="_blank" rel="noopener">http://www.dengshenyu.com/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/11/06/kafka-Meet-Kafka.html</a> - Kafka系列（一）初识Kafka</p>
</li>
<li><p><a href="https://lotabout.me/2018/kafka-introduction/" target="_blank" rel="noopener">https://lotabout.me/2018/kafka-introduction/</a> - Kafka 入门介绍</p>
</li>
<li><p><a href="https://www.zhihu.com/question/28925721" target="_blank" rel="noopener">https://www.zhihu.com/question/28925721</a> - Kafka 中的 Topic 为什么要进行分区? - 知乎</p>
</li>
<li><p><a href="https://blog.joway.io/posts/kafka-design-practice/" target="_blank" rel="noopener">https://blog.joway.io/posts/kafka-design-practice/</a> - Kafka 的设计与实践思考</p>
</li>
<li><p><a href="http://www.dengshenyu.com/分布式系统/2017/11/21/kafka-data-delivery.html" target="_blank" rel="noopener">http://www.dengshenyu.com/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/11/21/kafka-data-delivery.html</a> - Kafka系列（六）可靠的数据传输</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486269&idx=2&sn=ec00417ad641dd8c3d145d74cafa09ce&chksm=cea244f6f9d5cde0c8eb233fcc4cf82e11acd06446719a7af55230649863a3ddd95f78d111de&token=1633957262&lang=zh_CN#rd" target="_blank" rel="noopener">Kafka系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列?</a></p>
</li>
<li><p><a href="https://segmentfault.com/a/1190000012990954" target="_blank" rel="noopener">CentOS7 下 Kafka 的安装介绍 - 个人文章 - SegmentFault 思否</a></p>
</li>
<li><p>非常重要：<a href="https://blog.csdn.net/qq_25868207/article/details/81516024" target="_blank" rel="noopener">kafka 踩坑之消费者收不到消息 - kris - CSDN 博客</a></p>
</li>
<li><p><a href="https://blog.csdn.net/u010391342/article/details/81430402" target="_blank" rel="noopener">kafka 安装搭建(整合 springBoot 使用) - u010391342 的博客 - CSDN 博客</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/ALittleMoreLove/archive/2018/07/31/9396745.html" target="_blank" rel="noopener">Zookeeper+Kafka 的单节点配置 - 紫轩弦月 - 博客园</a></p>
</li>
<li><p><a href="https://blog.csdn.net/sweatOtt/article/details/86714272" target="_blank" rel="noopener">@KafkaListener 注解解密 - laomei - CSDN 博客</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/ac03f126980e" target="_blank" rel="noopener">使用Docker快速搭建Kafka开发环境 - 简书</a></p>
</li>
<li><p><a href="https://docs.spring.io/spring-kafka/reference/html" target="_blank" rel="noopener">Spring for Apache Kafka</a></p>
</li>
<li><p><a href="https://docs.spring.io/spring-kafka/docs/2.2.8.RELEASE/api/" target="_blank" rel="noopener">Overview (Spring Kafka 2.2.8.RELEASE API)</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/7a284bf4efc9" target="_blank" rel="noopener">SpringBoot整合Kafka实现发布订阅 - 简书</a></p>
</li>
<li><p><a href="http://www.mydlq.club/article/34/" target="_blank" rel="noopener">SpringBoot 集成 Spring For Kafka 操作 Kafka 详解 · 小豆丁个人博客</a></p>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/08/07/ShiftJava/K%20%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/B%20%E7%B3%BB%E7%BB%9F%E8%AE%A4%E8%AF%81%E4%B8%8E%E6%8E%88%E6%9D%83/" rel="next" title="ShiftJava/K 分布式与系统设计/B 系统认证与授权">
                <i class="fa fa-chevron-left"></i> ShiftJava/K 分布式与系统设计/B 系统认证与授权
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/08/07/ShiftJava/L%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/C%20RocketMQ/" rel="prev" title="ShiftJava/L 消息队列/C RocketMQ">
                ShiftJava/L 消息队列/C RocketMQ <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yue</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">163</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka"><span class="nav-number">1.</span> <span class="nav-text">Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基础"><span class="nav-number">1.1.</span> <span class="nav-text">基础</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-概述"><span class="nav-number">1.1.1.</span> <span class="nav-text">1. 概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-主题与分区"><span class="nav-number">1.1.2.</span> <span class="nav-text">2. 主题与分区</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-Broker与集群"><span class="nav-number">1.1.3.</span> <span class="nav-text">3. Broker与集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-多集群"><span class="nav-number">1.1.4.</span> <span class="nav-text">4. 多集群</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka架构模型"><span class="nav-number">1.2.</span> <span class="nav-text">Kafka架构模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka与存储实现"><span class="nav-number">1.3.</span> <span class="nav-text">Kafka与存储实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Kafka与文件系统"><span class="nav-number">1.3.1.</span> <span class="nav-text">1. Kafka与文件系统</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-底层存储细节"><span class="nav-number">1.3.2.</span> <span class="nav-text">2. 底层存储细节</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生产者"><span class="nav-number">1.4.</span> <span class="nav-text">生产者</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-写消息流程"><span class="nav-number">1.4.1.</span> <span class="nav-text">1. 写消息流程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#消费者"><span class="nav-number">1.5.</span> <span class="nav-text">消费者</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-消费者与消费组"><span class="nav-number">1.5.1.</span> <span class="nav-text">1. 消费者与消费组</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-消费组与分区重平衡"><span class="nav-number">1.5.2.</span> <span class="nav-text">2. 消费组与分区重平衡</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-Partition与消费模型"><span class="nav-number">1.5.3.</span> <span class="nav-text">3. Partition与消费模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-Kafka与pull模型"><span class="nav-number">1.5.4.</span> <span class="nav-text">4. Kafka与pull模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-Kafka如何保证消息消费顺序"><span class="nav-number">1.5.5.</span> <span class="nav-text">5. Kafka如何保证消息消费顺序</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-如何保证消息不丢失-可靠性"><span class="nav-number">1.5.6.</span> <span class="nav-text">6. 如何保证消息不丢失/可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-生产者丢失消息"><span class="nav-number">1.5.6.1.</span> <span class="nav-text">(1) 生产者丢失消息</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-消费者丢失消息"><span class="nav-number">1.5.6.2.</span> <span class="nav-text">(2) 消费者丢失消息</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-Kafka弄丢消息"><span class="nav-number">1.5.6.3.</span> <span class="nav-text">(3) Kafka弄丢消息</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-如何保证消息不被重复消费"><span class="nav-number">1.5.7.</span> <span class="nav-text">7. 如何保证消息不被重复消费</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-重复消费问题"><span class="nav-number">1.5.7.1.</span> <span class="nav-text">(1) 重复消费问题</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-保证幂等性"><span class="nav-number">1.5.7.2.</span> <span class="nav-text">(2) 保证幂等性</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-offset的维护"><span class="nav-number">1.5.8.</span> <span class="nav-text">8. offset的维护</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#可靠性"><span class="nav-number">1.6.</span> <span class="nav-text">可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-概述-1"><span class="nav-number">1.6.1.</span> <span class="nav-text">1. 概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-多副本机制"><span class="nav-number">1.6.2.</span> <span class="nav-text">2. 多副本机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-副本数据同步策略"><span class="nav-number">1.6.3.</span> <span class="nav-text">3. 副本数据同步策略</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka与Zookeeper"><span class="nav-number">1.7.</span> <span class="nav-text">Kafka与Zookeeper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka高效读写数据"><span class="nav-number">1.8.</span> <span class="nav-text">Kafka高效读写数据</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-顺序写磁盘"><span class="nav-number">1.8.1.</span> <span class="nav-text">1. 顺序写磁盘</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-零拷贝技术"><span class="nav-number">1.8.2.</span> <span class="nav-text">2. 零拷贝技术</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka事务"><span class="nav-number">1.9.</span> <span class="nav-text">Kafka事务</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Producer事务"><span class="nav-number">1.9.1.</span> <span class="nav-text">1. Producer事务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-Consumer事务"><span class="nav-number">1.9.2.</span> <span class="nav-text">2. Consumer事务</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Producer-API"><span class="nav-number">1.10.</span> <span class="nav-text">Producer API</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-消息发送流程"><span class="nav-number">1.10.1.</span> <span class="nav-text">1. 消息发送流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-异步发送API"><span class="nav-number">1.10.2.</span> <span class="nav-text">2. 异步发送API</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-不带回调函数的异步（AsyncProducer）"><span class="nav-number">1.10.2.1.</span> <span class="nav-text">(1) 不带回调函数的异步（AsyncProducer）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-带回调函数的异步（CallbackProducer）"><span class="nav-number">1.10.2.2.</span> <span class="nav-text">(2) 带回调函数的异步（CallbackProducer）</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-同步发送API"><span class="nav-number">1.10.3.</span> <span class="nav-text">3. 同步发送API</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-同步发送（SyncProducer）"><span class="nav-number">1.10.3.1.</span> <span class="nav-text">(1) 同步发送（SyncProducer）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Consumer-API"><span class="nav-number">1.11.</span> <span class="nav-text">Consumer API</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-自动提交offset"><span class="nav-number">1.11.1.</span> <span class="nav-text">1. 自动提交offset</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-手动提交offset"><span class="nav-number">1.11.2.</span> <span class="nav-text">2. 手动提交offset</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-同步提交commitSync-offset"><span class="nav-number">1.11.2.1.</span> <span class="nav-text">(1) 同步提交commitSync offset</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-异步提交commitAsync-offset"><span class="nav-number">1.11.2.2.</span> <span class="nav-text">(2) 异步提交commitAsync offset</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-数据漏消费和重复消费分析"><span class="nav-number">1.11.2.3.</span> <span class="nav-text">(3) 数据漏消费和重复消费分析</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-自定义存储offset"><span class="nav-number">1.11.3.</span> <span class="nav-text">3. 自定义存储offset</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spring整合Kafka"><span class="nav-number">1.12.</span> <span class="nav-text">Spring整合Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Producer"><span class="nav-number">1.12.1.</span> <span class="nav-text">1. Producer</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-KafkaProducer"><span class="nav-number">1.12.1.1.</span> <span class="nav-text">(1) KafkaProducer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-KafkaProducerConfig"><span class="nav-number">1.12.1.2.</span> <span class="nav-text">(2) KafkaProducerConfig</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-Consumer"><span class="nav-number">1.12.2.</span> <span class="nav-text">2. Consumer</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-KafkaConsumer"><span class="nav-number">1.12.2.1.</span> <span class="nav-text">(1) KafkaConsumer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-KafkaConsumerConfig"><span class="nav-number">1.12.2.2.</span> <span class="nav-text">(2) KafkaConsumerConfig</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-配置文件"><span class="nav-number">1.12.3.</span> <span class="nav-text">3. 配置文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-pom依赖"><span class="nav-number">1.12.4.</span> <span class="nav-text">4. pom依赖</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参考资料"><span class="nav-number">1.13.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yue</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
